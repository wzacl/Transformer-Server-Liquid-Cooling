# /usr/bin/python3
'''''
GB_PID_pump.py

GB_PIDæ§åˆ¶å™¨ï¼Œåˆ©ç”¨Guaranteed Bounded PIDæ§åˆ¶æ³µè½‰é€Ÿ

æœ¬ç ”ç©¶ä¸­çš„æ™¶ç‰‡ç“¦æ•¸å°æ‡‰çš„é›»æºä¾›æ‡‰å™¨åƒæ•¸è¨­ç½®å¦‚ä¸‹
1KWï¼š220V_8A
1.5KWï¼š285V_8A
1.9KWï¼š332V_8A

å°æ‡‰çš„é¢¨æ‰‡èˆ‡æ³µæœ€ä½è½‰é€Ÿå¦‚ä¸‹
æ³µï¼š40% duty cycle
é¢¨æ‰‡ï¼š30% duty cycle
'''''
import time
import sys
sys.path.append('/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/code_manage/Control_Unit')
sys.path.append('/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/code_manage/Controllers/MPC/Model_constructor')
import ADAMScontroller
import pwmcontroller as ctrl
import multi_channel_pwmcontroller as multi_ctrl
from simple_pid import PID
import matplotlib.pyplot as plt
import numpy as np
import time
import joblib
import Transformer_enc_dec
import torch
import torch.nn as nn
from collections import deque
import math
from sklearn.preprocessing import MinMaxScaler
import os
import pandas as pd
import csv
import random
import Model_tester as mt
import Sequence_Window_Processor
from tabulate import tabulate

adam_port = '/dev/ttyUSB0'
fan1_port = '/dev/ttyAMA4'
fan2_port = '/dev/ttyAMA5'
pump_port = '/dev/ttyAMA3'
#é¸æ“‡æ¨¡å‹
test_model='multi_seq35_steps8_batch512_hidden16_encoder1_decoder1_heads2_dropout0.005_epoch200'

# å¾æ¨¡å‹åç¨±ä¸­æå–è¶…åƒæ•¸
model_params = {}
params_str = test_model.split('_')
for param in params_str:
    if 'seq' in param:
        model_params['seq_len'] = int(param.replace('seq', ''))
    elif 'hidden' in param:
        model_params['hidden_dim'] = int(param.replace('hidden', ''))
    elif 'encoder' in param:
        model_params['num_encoder_layers'] = int(param.replace('encoder', ''))
    elif 'decoder' in param:
        model_params['num_decoder_layers'] = int(param.replace('decoder', ''))
    elif 'heads' in param:
        model_params['num_heads'] = int(param.replace('heads', ''))
    elif 'dropout' in param:
        model_params['dropout'] = float(param.replace('dropout', ''))

# æ›´æ–°æ™‚é–“çª—å£å¤§å°
time_window = model_params['seq_len']

#è¨­ç½®å¯¦é©—è³‡æ–™æ”¾ç½®çš„è³‡æ–™å¤¾
exp_name = f'/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/data_manage/Real_time_Prediction_data/only1.5KW_USE_T_env/{test_model}'
#è¨­ç½®å¯¦é©—è³‡æ–™æª”æ¡ˆåç¨±
exp_var = 'GPU15KW_1(285V_8A)_fan_test'
#è¨­ç½®å¯¦é©—è³‡æ–™æ¨™é¡Œ
custom_headers = ['time', 'T_GPU', 'T_heater', 'T_CDU_in', 'T_CDU_out', 'T_env', 'T_air_in', 'T_air_out', 'TMP8', 'fan_duty', 'pump_duty', 'GPU_Watt(KW)']

adam = ADAMScontroller.DataAcquisition(exp_name=exp_name, exp_var=exp_var, port=adam_port, csv_headers=custom_headers)
fan1 = multi_ctrl.multichannel_PWMController(fan1_port)
fan2 = multi_ctrl.multichannel_PWMController(fan2_port)
pump = ctrl.XYKPWMController(pump_port)
print('æ¨¡å‹åˆå§‹åŒ–.....')


# ä¿®æ”¹æ¨¡å‹å’Œscalerè·¯å¾‘
model_path = f'/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/code_manage/Predict_Model/{test_model}/2KWCDU_Transformer_model.pth'
# è¨­å®šMinMaxScalerçš„è·¯å¾‘ï¼Œæ­¤scalerç”¨æ–¼å°‡è¼¸å…¥æ•¸æ“šæ­¸ä¸€åŒ–åˆ°[0,1]å€é–“
# è©²scaleræ˜¯åœ¨è¨“ç·´æ¨¡å‹æ™‚ä¿å­˜çš„ï¼Œç¢ºä¿é æ¸¬æ™‚ä½¿ç”¨ç›¸åŒçš„æ•¸æ“šç¸®æ”¾æ–¹å¼
scaler_path = f'/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/code_manage/Predict_Model/{test_model}/1.5_1KWscalers.jlib' 
# æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨,å¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºä¸¦å¯«å…¥æ¨™é¡Œè¡Œ
prediction_file = f'/home/inventec/Desktop/2KWCDU_ä¿®æ”¹ç‰ˆæœ¬/data_manage/Real_time_Prediction/only1.5KW_USE_T_env/{test_model}/Model_test_{exp_var}.csv'
if not os.path.exists(prediction_file):
    os.makedirs(os.path.dirname(prediction_file), exist_ok=True)
    with open(prediction_file, 'w') as f:
        f.write('timestamp,actual_temp(CDU_out),actual_temp(GPU),fan_duty,pump_duty,pred_1,pred_2,pred_3,pred_4,pred_5,pred_6,pred_7,pred_8\n')

# è¨­ç½®åˆå§‹è½‰é€Ÿ
pump_duty=60
pump.set_duty_cycle(pump_duty)
fan_duty=60
fan1.set_all_duty_cycle(fan_duty)
fan2.set_all_duty_cycle(fan_duty)

# è¨­ç½®ADAMæ§åˆ¶å™¨
adam.start_adam()
adam.update_duty_cycles(fan_duty, pump_duty)
time.sleep(2)

# åŠ è¼‰æ¨¡å‹å’Œscaler
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_state_dict = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)
model = Transformer_enc_dec.TransformerModel(
    input_dim=7, 
    hidden_dim=model_params['hidden_dim'], 
    output_dim=1, 
    num_encoder_layers=model_params['num_encoder_layers'], 
    num_decoder_layers=model_params['num_decoder_layers'], 
    num_heads=model_params['num_heads'], 
    dropout=model_params['dropout']
)
model.load_state_dict(model_state_dict)
model.eval()

seq_window_processor = Sequence_Window_Processor.SequenceWindowProcessor(
    window_size=time_window,
    adams_controller=adam,  # ä½ çš„ ADAMScontroller ç‰©ä»¶
    scaler_path=scaler_path,  # ä½ çš„ Scaler æª”æ¡ˆ
    device="cpu"
)



# ä¿®æ”¹é æ¸¬æ•¸æ“šè¨˜éŒ„çµæ§‹
prediction_data = {
    'timestamp': [],
    'actual_temps(CDU_out)': [],
    'actual_temps(GPU)': [],
    'T_env': [],
    'fan_duty': [],
    'pump_duty': [],
    'predicted_sequence': []  # å„²å­˜8å€‹æ™‚é–“æ­¥çš„é æ¸¬
}

# å‰µå»º Model_tester ç‰©ä»¶
model_tester = mt.Model_tester(fan1=fan1, fan2=fan2, pump=pump, adam=adam)

# é¸æ“‡æ¸¬è©¦æ¨¡å¼ (1: åªè®Šå‹•é¢¨æ‰‡, 2: åªè®Šå‹•æ³µ, 3: éš¨æ©Ÿè®Šå‹•)
model_tester.start_test(1)  # é€™è£¡é¸æ“‡éš¨æ©Ÿè®Šå‹•æ¸¬è©¦


while model_tester.phase != "end":
    try:
        model_tester.update_test()

        # âœ… æ›´æ–°ä¾†è‡ª ADAMS çš„æ•¸æ“šï¼Œç¢ºä¿æ»‘å‹•çª—å£æ•¸æ“šæ˜¯æœ€æ–°çš„
        #seq_window_processor.update_from_adam()

        # âœ… ç¢ºä¿ window_data å·²æº–å‚™å¥½
        input_tensor = seq_window_processor.get_window_data(normalize=True)

        if input_tensor is None:  # ä¿®æ­£æ¢ä»¶ï¼Œæ‡‰è©²ç­‰å¾…æ•¸æ“šæº–å‚™å¥½
            time.sleep(1)
            continue

        # ç²å–ç•¶å‰æ•¸æ“š
        data = [
            adam.buffer[0],  # T_GPU
            adam.buffer[2],  # T_CDU_in
            adam.buffer[3],  # T_CDU_out
            adam.buffer[4],  # T_env
            adam.buffer[5],  # T_air_in
            adam.buffer[6],  # T_air_out
            adam.buffer[8],  # fan_duty
            adam.buffer[9]   # pump_duty
        ]

        # åŸ·è¡Œé æ¸¬
        with torch.no_grad():
            inference_start_time = time.time()
            scaled_predictions = model(input_tensor, num_steps=8)[0].cpu().numpy()
            inference_end_time = time.time()
            inference_duration = inference_end_time - inference_start_time

        # å°‡é æ¸¬çµæœè½‰æ›å›åŸå§‹ç¯„åœ
        predicted_sequence = seq_window_processor.inverse_transform_predictions(scaled_predictions.reshape(-1, 1),smooth=True).flatten()

        # è¨˜éŒ„çµæœ
        current_time = time.time()
        keys = ['timestamp', 'actual_temps(CDU_out)', 'actual_temps(GPU)', 'T_env', 'fan_duty', 'pump_duty', 'predicted_sequence']
        values = [current_time, adam.buffer[3], data[0], data[2], adam.buffer[8], adam.buffer[9], predicted_sequence]
        for key, value in zip(keys, values):
            prediction_data[key].append(value)

        # å¯«å…¥é æ¸¬æ•¸æ“šåˆ° CSV æª”æ¡ˆ
        with open(prediction_file, 'a', newline='') as f:
            writer = csv.writer(f)
            timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))
            row = [timestamp, adam.buffer[3], data[0], adam.buffer[8], adam.buffer[9]] + [f'{temp:.2f}' for temp in predicted_sequence]
            writer.writerow(row)

        # ç³»çµ±ç‹€æ…‹æ•¸æ“š
        system_status_data = [
            ["ğŸŒ¡ï¸ ç•¶å‰å‡ºå£æº«åº¦", f"{adam.buffer[3]:.2f}Â°C"],
            ["ğŸ’» ç•¶å‰æ™¶ç‰‡æº«åº¦", f"{data[0]:.2f}Â°C"],
            ["ğŸŒ ç•¶å‰ç’°å¢ƒæº«åº¦", f"{data[3]:.2f}Â°C"],
            ["ğŸ’¨ ç•¶å‰é€²é¢¨æº«åº¦", f"{data[4]:.2f}Â°C"],
            ["ğŸŒ¬ï¸ ç•¶å‰å‡ºé¢¨æº«åº¦", f"{data[5]:.2f}Â°C"],
            ["ğŸ”„ ç•¶å‰é¢¨æ‰‡è½‰é€Ÿ", f"{data[6]:.2f}%"],
            ["ğŸ’§ ç•¶å‰æ³µè½‰é€Ÿ", f"{data[7]:.2f}%"]
        ]

        # é æ¸¬çµæœæ•¸æ“š
        prediction_results_data = [
            ["ğŸ”® æœªä¾†8æ­¥é æ¸¬æº«åº¦", predicted_sequence.tolist()],  # ä¿®æ­£ç‚ºåˆ—è¡¨ï¼Œä»¥ç¢ºä¿å¯è®€æ€§
            ["ğŸ“ scaled_predictions å½¢ç‹€", scaled_predictions.shape],
            ["â±ï¸ æ¨¡å‹æ¨è«–æ™‚é–“", f"{inference_duration:.4f} ç§’"]
        ]

        # æ‰“å°ç³»çµ±ç‹€æ…‹è¡¨æ ¼
        print("ğŸŒŸ==================== ç³»çµ±ç‹€æ…‹ ====================ğŸŒŸ")
        print(tabulate(system_status_data, tablefmt="grid"))

        # æ‰“å°é æ¸¬çµæœè¡¨æ ¼
        print("\nğŸŒŸ==================== é æ¸¬çµæœ ====================ğŸŒŸ")
        print(tabulate(prediction_results_data, tablefmt="grid"))
        
        time.sleep(1)

    except ValueError as e:
        print(f"âš ï¸ éŒ¯èª¤: {str(e)}")
        time.sleep(1)

    except KeyboardInterrupt:
        print("å¯¦é©—çµæŸï¼Œç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")
        adam.stop_adam()
        
        break

    except Exception as e:
        print(f"âŒ é æ¸¬éŒ¯èª¤: {str(e)}")
        time.sleep(1)
adam.stop_adam()
fan1.set_all_duty_cycle(60)
fan2.set_all_duty_cycle(60)
pump.set_duty_cycle(60)
print("ğŸ”´ å¯¦é©—çµæŸï¼Œç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")


adam.stop_adam()
fan1.set_all_duty_cycle(60)
fan2.set_all_duty_cycle(60)
pump.set_duty_cycle(60)






